import json
from datetime import datetime, timedelta
import re

class QuantumMemory:
    def __init__(self, chat):
        self.chat = chat
        self.timestamp = "2025-03-10 09:34:43"
        self.user = "artgalleryma2ta"
        
        # Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ ÙØ§ÛŒÙ„
        self.history_file = f"{chat.chat_dir}/chat_history.json"
        self.learning_file = f"{chat.knowledge_dir}/learned_data.json"
        self.stats_file = f"{chat.knowledge_dir}/memory_stats.json"
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        self.load_data()
    
    def load_data(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²"""
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú†Øª
        self.history = self._load_json(self.history_file, {
            "metadata": {
                "created_at": self.timestamp,
                "user": self.user,
                "version": self.chat.version,
                "last_update": self.timestamp
            },
            "conversations": []
        })
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡
        self.learned = self._load_json(self.learning_file, {
            "topics": {},
            "patterns": {},
            "last_learn": self.timestamp,
            "stats": {
                "total_topics": 0,
                "total_patterns": 0,
                "last_cleanup": self.timestamp
            }
        })
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¢Ù…Ø§Ø± Ø­Ø§ÙØ¸Ù‡
        self.stats = self._load_json(self.stats_file, {
            "total_messages": 0,
            "unique_topics": 0,
            "learned_patterns": 0,
            "successful_responses": 0,
            "last_update": self.timestamp,
            "monthly_stats": {},
            "topic_frequency": {}
        })
    
    def _load_json(self, file_path, default_data):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ JSON Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ {file_path}: {str(e)}")
            return default_data
    
    def _save_json(self, file_path, data):
        """Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ JSON Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=4, ensure_ascii=False)
            return True
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ {file_path}: {str(e)}")
            return False
    
    def save_conversation(self, message, response):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¬Ø¯ÛŒØ¯"""
        # Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡
        self.history["conversations"].extend([
            {
                "timestamp": self.timestamp,
                "user": self.user,
                "message": message,
                "type": "user"
            },
            {
                "timestamp": self.timestamp,
                "user": "assistant",
                "message": response,
                "type": "assistant"
            }
        ])
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
        self.stats["total_messages"] += 2
        self._update_topic_stats(message)
        
        # Ø°Ø®ÛŒØ±Ù‡ ØªØºÛŒÛŒØ±Ø§Øª
        self._save_json(self.history_file, self.history)
        self._save_json(self.stats_file, self.stats)
    
    def learn_from_web(self, query, results):
        """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨"""
        topic = query.lower()
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¬Ø¯ÛŒØ¯
        self.learned["topics"][topic] = {
            "timestamp": self.timestamp,
            "source": "web_search",
            "data": results,
            "keywords": self._extract_keywords(query),
            "patterns": self._extract_patterns(results)
        }
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
        self.learned["stats"]["total_topics"] += 1
        self.stats["unique_topics"] += 1
        
        # Ø°Ø®ÛŒØ±Ù‡ ØªØºÛŒÛŒØ±Ø§Øª
        self._save_json(self.learning_file, self.learned)
        self._save_json(self.stats_file, self.stats)
    
    def find_response(self, query):
        """ÛŒØ§ÙØªÙ† Ù¾Ø§Ø³Ø® Ù…Ù†Ø§Ø³Ø¨ Ø§Ø² Ø­Ø§ÙØ¸Ù‡"""
        best_match = None
        max_similarity = 0
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÛŒØ§Ø¯Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡
        query_keywords = set(self._extract_keywords(query))
        
        for topic, data in self.learned["topics"].items():
            topic_keywords = set(data["keywords"])
            similarity = self._calculate_similarity(query_keywords, topic_keywords)
            
            if similarity > max_similarity and similarity > 0.3:  # Ø­Ø¯Ø§Ù‚Ù„ 30% Ø´Ø¨Ø§Ù‡Øª
                max_similarity = similarity
                best_match = data
        
        if best_match:
            self.stats["successful_responses"] += 1
            self._save_json(self.stats_file, self.stats)
            
            return self._format_response(best_match["data"])
        
        return None
    
    def get_recent_conversations(self, limit=100):
        """Ø¯Ø±ÛŒØ§ÙØª Ù…Ú©Ø§Ù„Ù…Ø§Øª Ø§Ø®ÛŒØ±"""
        return self.history["conversations"][-limit:]
    
    def get_old_topics(self, days=7):
        """Ø¯Ø±ÛŒØ§ÙØª Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù‚Ø¯ÛŒÙ…ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ"""
        old_topics = []
        current_time = datetime.now()
        
        for topic, data in self.learned["topics"].items():
            topic_time = datetime.strptime(data["timestamp"], "%Y-%m-%d %H:%M:%S")
            if (current_time - topic_time).days > days:
                old_topics.append(topic)
        
        return old_topics
    
    def _extract_keywords(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ù…ØªÙ†"""
        # Ø­Ø°Ù Ú©Ù„Ù…Ø§Øª Ø§Ø¶Ø§ÙÛŒ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ù…Ù‡Ù…
        stop_words = {"Ùˆ", "Ø¯Ø±", "Ø¨Ù‡", "Ø§Ø²", "Ú©Ù‡", "Ø§ÛŒÙ†", "Ø±Ø§", "Ø¨Ø§", "Ø§Ø³Øª", "Ø¨Ø±Ø§ÛŒ", "Ø¢Ù†"}
        words = text.lower().split()
        return [word for word in words if len(word) > 2 and word not in stop_words]
    
    def _extract_patterns(self, results):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø§Ø² Ù†ØªØ§ÛŒØ¬"""
        patterns = []
        for result in results:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ù‡Ù… Ø§Ø² Ø¹Ù†ÙˆØ§Ù† Ùˆ ØªÙˆØ¶ÛŒØ­Ø§Øª
            text = f"{result['title']} {result['snippet']}"
            # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø¹Ù…ÙˆÙ„ Ù…Ø«Ù„: ØªØ¹Ø§Ø±ÛŒÙØŒ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ØŒ Ø±ÙˆØ´â€ŒÙ‡Ø§
            definitions = re.findall(r'([^.!?]+(Ø§Ø³Øª|Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯|ÛŒØ¹Ù†ÛŒ)[^.!?]+)[.!?]', text)
            examples = re.findall(r'([^.!?]+(Ù…Ø«Ù„Ø§Ù‹|Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„|Ù…Ø§Ù†Ù†Ø¯)[^.!?]+)[.!?]', text)
            methods = re.findall(r'([^.!?]+(Ø±ÙˆØ´|Ú†Ú¯ÙˆÙ†Ù‡|Ú†Ø·ÙˆØ±)[^.!?]+)[.!?]', text)
            
            patterns.extend([d[0] for d in definitions])
            patterns.extend([e[0] for e in examples])
            patterns.extend([m[0] for m in methods])
        
        return patterns
    
    def _calculate_similarity(self, set1, set2):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒÙ† Ø¯Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ù‡"""
        if not set1 or not set2:
            return 0
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        return intersection / union
    
    def _format_response(self, results):
        """Ù‚Ø§Ù„Ø¨â€ŒØ¨Ù†Ø¯ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®"""
        response = "Ø¨Ø±Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡:\n\n"
        for result in results[:2]:  # Ù†Ù…Ø§ÛŒØ´ 2 Ù†ØªÛŒØ¬Ù‡ Ø¨Ø±ØªØ±
            response += f"ğŸ”¹ {result['title']}\n{result['snippet']}\n\n"
        return response
    
    def _update_topic_stats(self, message):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø± Ù…ÙˆØ¶ÙˆØ¹Ø§Øª"""
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø± Ù…Ø§Ù‡Ø§Ù†Ù‡
        current_month = datetime.now().strftime("%Y-%m")
        if current_month not in self.stats["monthly_stats"]:
            self.stats["monthly_stats"][current_month] = {
                "messages": 0,
                "topics": {},
                "patterns": 0
            }
        
        self.stats["monthly_stats"][current_month]["messages"] += 1
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
        keywords = self._extract_keywords(message)
        for keyword in keywords:
            if keyword not in self.stats["topic_frequency"]:
                self.stats["topic_frequency"][keyword] = 0
            self.stats["topic_frequency"][keyword] += 1
    
    def cleanup_old_data(self, days=30):
        """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        current_time = datetime.now()
        cleanup_date = current_time - timedelta(days=days)
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…Ú©Ø§Ù„Ù…Ø§Øª Ù‚Ø¯ÛŒÙ…ÛŒ
        self.history["conversations"] = [
            conv for conv in self.history["conversations"]
            if datetime.strptime(conv["timestamp"], "%Y-%m-%d %H:%M:%S") > cleanup_date
        ]
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù‚Ø¯ÛŒÙ…ÛŒ Ú©Ù…â€ŒØ§Ø³ØªÙØ§Ø¯Ù‡
        for topic in list(self.learned["topics"].keys()):
            topic_data = self.learned["topics"][topic]
            topic_time = datetime.strptime(topic_data["timestamp"], "%Y-%m-%d %H:%M:%S")
            if topic_time < cleanup_date:
                # Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒØ²Ø§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡
                if topic.lower() in self.stats["topic_frequency"]:
                    if self.stats["topic_frequency"][topic.lower()] < 5:  # Ú©Ù…ØªØ± Ø§Ø² 5 Ø¨Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡
                        del self.learned["topics"][topic]
                else:
                    del self.learned["topics"][topic]
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø²Ù…Ø§Ù† Ø¢Ø®Ø±ÛŒÙ† Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
        self.learned["stats"]["last_cleanup"] = self.timestamp
        
        # Ø°Ø®ÛŒØ±Ù‡ ØªØºÛŒÛŒØ±Ø§Øª
        self._save_json(self.history_file, self.history)
        self._save_json(self.learning_file, self.learned)
    
    def get_memory_stats(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø­Ø§ÙØ¸Ù‡"""
        return {
            "total_messages": self.stats["total_messages"],
            "unique_topics": self.stats["unique_topics"],
            "successful_responses": self.stats["successful_responses"],
            "topics_count": len(self.learned["topics"]),
            "patterns_count": self.learned["stats"]["total_patterns"],
            "last_update": self.timestamp,
            "monthly_stats": self.stats["monthly_stats"],
            "most_common_topics": sorted(
                self.stats["topic_frequency"].items(),
                key=lambda x: x[1],
                reverse=True
            )[:10]  # 10 Ù…ÙˆØ¶ÙˆØ¹ Ù¾Ø±ØªÚ©Ø±Ø§Ø±
        }